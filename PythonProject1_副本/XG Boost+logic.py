# -*- coding: utf-8 -*-
"""
ICU Mortality Prediction â€” Logistic Regression & (optional) XGBoost
pip install -U pandas numpy scikit-learn matplotlib seaborn xgboost
"""


from pathlib import Path
import argparse, re, warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.calibration import CalibrationDisplay
from sklearn.linear_model import LogisticRegression

warnings.filterwarnings("ignore")
try:
    from xgboost import XGBClassifier
    HAS_XGB = True
except Exception:
    HAS_XGB = False
    print("[Note] xgboost æœªå®‰è£…ï¼Œå¯é€šè¿‡ `pip install xgboost` å®‰è£…ã€‚")

# -------------------- è·¯å¾„ä¸å‚æ•° --------------------
SCRIPT_DIR = Path(__file__).resolve().parent
DATA_PATH        = SCRIPT_DIR / "processed_icu_data.csv"
UNIVARIATE_PATH  = SCRIPT_DIR / "univariate_analysis_results.csv"
CORRELATION_PATH = SCRIPT_DIR / "correlation_analysis_results.csv"

parser = argparse.ArgumentParser()
parser.add_argument("--target", type=str, default=None,
                    help="æ ‡ç­¾åˆ—åï¼ˆå¦‚ In-hospital_death / hospital_expire_flag / outcome ç­‰ï¼‰")
args, _ = parser.parse_known_args()

print("å½“å‰è¿è¡Œç›®å½•:", Path.cwd())
print("è„šæœ¬æ‰€åœ¨ç›®å½•:", SCRIPT_DIR)
print("æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨:", DATA_PATH.exists())

# -------------------- è¯»æ•°æ® --------------------
df = pd.read_csv(DATA_PATH)
print(f"âœ… æˆåŠŸè¯»å–æ•°æ®ï¼Shape: {df.shape}")
print("å‰20ä¸ªåˆ—åï¼š", list(df.columns)[:20])

# -------------------- ç›®æ ‡åˆ—é€‰æ‹©ä¸æ ‡å‡†åŒ– --------------------
def normalize_label_series(s: pd.Series) -> pd.Series:
    """å°†å„ç§å†™æ³•ç»Ÿä¸€æˆ0/1"""
    if s.dtype.kind in "biu":  # å·²ç»æ˜¯æ•°å€¼å‹
        return s.astype(int)
    s_str = s.astype(str).str.strip().str.lower()
    out = pd.Series(np.nan, index=s.index, dtype="float")
    pos = r"^(1|true|yes|y|dead|deceased|expired?|æ­»äº¡)$"
    neg = r"^(0|false|no|n|alive|surviv(e|ed)|discharged?|å­˜æ´»)$"
    out[s_str.str.match(pos, na=False)] = 1
    out[s_str.str.match(neg, na=False)] = 0
    if out.isna().any():
        with np.errstate(all="ignore"):
            num = pd.to_numeric(s, errors="coerce")
        out = out.fillna(num)
    return out.astype(float)

def pick_target_column(df: pd.DataFrame, prefer: str | None = None) -> str | None:
    # å…è®¸ä¼˜å…ˆæ‰‹åŠ¨æŒ‡å®š
    if prefer and prefer in df.columns:
        return prefer
    # å¸¸è§åå­—ï¼ˆæ³¨æ„å«è¿å­—ç¬¦çš„ In-hospital_deathï¼‰
    candidates = [
        "in-hospital_death", "in_hospital_death", "inhospital_death",
        "hospital_expire_flag", "in_hospital_mortality", "icu_mortality",
        "mortality", "death", "deceased", "expire_flag", "outcome", "label", "y", "target"
    ]
    lower_map = {c.lower(): c for c in df.columns}
    for key in candidates:
        if key in lower_map:
            return lower_map[key]
    # æ¨¡ç³ŠåŒ¹é…
    patt = re.compile(r"(mort(al(ity)?)?|expir(e|ed|y)|death|deceas)", re.I)
    for c in df.columns:
        if patt.search(c):
            return c
    # äºŒå€¼åˆ—å…œåº•
    for c in df.columns:
        if df[c].nunique(dropna=True) <= 3:
            s = normalize_label_series(df[c])
            vals = set(pd.unique(s.dropna()))
            if vals <= {0.0, 1.0}:
                return c
    return None

target_col = pick_target_column(df, prefer=args.target)
if target_col is None:
    print("\nâŒ æ²¡æ‰¾åˆ°æ ‡ç­¾åˆ—ã€‚å¯ç”¨ `--target åˆ—å` æŒ‡å®šã€‚")
    print("å…¨éƒ¨åˆ—åï¼š", list(df.columns))
    raise SystemExit(1)

print(f"ğŸ¯ ä½¿ç”¨ç›®æ ‡åˆ—: {target_col}")

y = normalize_label_series(df[target_col]).round().astype(int).values
ID_COL = "RecordID" if "RecordID" in df.columns else None
drop_cols = [c for c in [target_col, ID_COL] if c in df.columns]
X = df.drop(columns=drop_cols)
print(f"ç‰¹å¾ç»´åº¦: {X.shape},  å‰”é™¤åˆ—: {drop_cols}")

# -------------------- EDAï¼ˆå¯é€‰ï¼‰ --------------------
print("\nğŸ“Š ç¼ºå¤±ç‡ï¼ˆå‰10ä¸ªç‰¹å¾ï¼‰ï¼š")
print(X.isna().mean().sort_values(ascending=False).head(10))

plt.figure()
sns.countplot(x=y)
plt.title("Target åˆ†å¸ƒ")
plt.show()

if CORRELATION_PATH.exists():
    corr_df = pd.read_csv(CORRELATION_PATH)
    if {"feature1","feature2","corr"} <= set(map(str.lower, corr_df.columns)):
        pivot = corr_df.pivot(index=corr_df.columns[0], columns=corr_df.columns[1], values=corr_df.columns[2])
        plt.figure(figsize=(10,8))
        sns.heatmap(pivot, cmap="coolwarm", center=0)
        plt.title("Correlation Heatmap (provided)")
        plt.show()
else:
    num_X = X.select_dtypes(include=[np.number]).iloc[:, :50]
    if num_X.shape[1] >= 2:
        plt.figure(figsize=(10,8))
        sns.heatmap(num_X.corr(), cmap="coolwarm", center=0)
        plt.title("Correlation Heatmap (auto subset)")
        plt.show()

# -------------------- åˆ’åˆ†æ•°æ® --------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print("\nTrain:", X_train.shape, " Test:", X_test.shape)

# -------------------- é¢„å¤„ç† --------------------
num_cols = list(X_train.select_dtypes(include=[np.number]).columns)
cat_cols = [c for c in X_train.columns if c not in num_cols]

num_pipe_logit = Pipeline([
    ("impute", SimpleImputer(strategy="median")),
    ("scale", StandardScaler())
])
cat_pipe = Pipeline([
    ("impute", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])
pre_logit = ColumnTransformer([
    ("num", num_pipe_logit, num_cols),
    ("cat", cat_pipe, cat_cols)
])

# -------------------- Logistic --------------------
logit = Pipeline([
    ("pre", pre_logit),
    ("clf", LogisticRegression(max_iter=2000, solver="lbfgs"))
])
logit.fit(X_train, y_train)
proba_logit = logit.predict_proba(X_test)[:, 1]
print(f"\n[Logit] AUROC={roc_auc_score(y_test, proba_logit):.4f} "
      f"| AUPRC={average_precision_score(y_test, proba_logit):.4f} "
      f"| Brier={brier_score_loss(y_test, proba_logit):.4f}")
print(classification_report(y_test, (proba_logit>=0.5).astype(int), digits=4))
sns.heatmap(confusion_matrix(y_test, (proba_logit>=0.5).astype(int)), annot=True, fmt="d", cbar=False)
plt.title("Logistic Confusion Matrix (0.5)")
plt.show()
CalibrationDisplay.from_predictions(y_test, proba_logit, n_bins=10)
plt.title("Logistic Calibration")
plt.show()

# -------------------- XGBoostï¼ˆå¯é€‰ï¼‰ --------------------
if HAS_XGB:
    pre_xgb = ColumnTransformer([
        ("num", SimpleImputer(strategy="median"), num_cols),
        ("cat", cat_pipe, cat_cols)
    ])
    xgb = Pipeline([
        ("pre", pre_xgb),
        ("clf", XGBClassifier(
            n_estimators=500, max_depth=6, learning_rate=0.05,
            subsample=0.8, colsample_bytree=0.8,
            reg_lambda=1.0, random_state=42,
            eval_metric="logloss", tree_method="hist"
        ))
    ])
    xgb.fit(X_train, y_train)
    proba_xgb = xgb.predict_proba(X_test)[:, 1]
    print(f"\n[XGB]  AUROC={roc_auc_score(y_test, proba_xgb):.4f} "
          f"| AUPRC={average_precision_score(y_test, proba_xgb):.4f} "
          f"| Brier={brier_score_loss(y_test, proba_xgb):.4f}")
    sns.heatmap(confusion_matrix(y_test, (proba_xgb>=0.5).astype(int)), annot=True, fmt="d", cbar=False)
    plt.title("XGBoost Confusion Matrix (0.5)")
    plt.show()
    CalibrationDisplay.from_predictions(y_test, proba_xgb, n_bins=10)
    plt.title("XGBoost Calibration")
    plt.show()

# -------------------- å¯¼å‡ºé¢„æµ‹ --------------------
out = pd.DataFrame({"y_true": y_test, "proba_logit": proba_logit})
try:
    out["proba_xgb"] = proba_xgb  # è‹¥æœªè®­ç»ƒXGBä¼šè·³è¿‡
except Exception:
    pass
out.to_csv(SCRIPT_DIR / "test_predictions.csv", index=False)
print("\nâœ… å·²ç”Ÿæˆ test_predictions.csv")

from sklearn.metrics import accuracy_score

# Logistic Regression çš„å‡†ç¡®ç‡
logit_preds = (proba_logit >= 0.5).astype(int)
acc_logit = accuracy_score(y_test, logit_preds)
print(f"ğŸ¯ Logistic Regression å‡†ç¡®ç‡ (Accuracy): {acc_logit:.4f}")

# è‹¥è®­ç»ƒäº† XGBoostï¼Œä¹Ÿè®¡ç®—å®ƒçš„å‡†ç¡®ç‡
if HAS_XGB:
    xgb_preds = (proba_xgb >= 0.5).astype(int)
    acc_xgb = accuracy_score(y_test, xgb_preds)
    print(f"ğŸŒ² XGBoost å‡†ç¡®ç‡ (Accuracy): {acc_xgb:.4f}")

plt.figure()
models = ["Logistic", "XGBoost"]
accs = [acc_logit, acc_xgb if HAS_XGB else np.nan]
plt.bar(models, accs, color=["#1f77b4", "#2ca02c"])
plt.ylim(0, 1)
plt.ylabel("Accuracy")
plt.title("æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”")
plt.show()
